{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem set 9: Statistics, feature selection, and feature importance\n",
    "\n",
    "## Summary\n",
    "\n",
    "Examine the differences between British and American fiction in the class-curated literary corpus. Apply statistical measures and calculate feature importance in a simple classifier.\n",
    "\n",
    "## Details\n",
    "\n",
    "You will work with a corpus of 131 volumes of fiction by British and American authors. These volumes are taken from the class corpus, so you'll need to download a copy of the texts from [Google Drive](https://drive.google.com/drive/folders/1lbeZiBAVCzjCWojCK8mfmELa-Q8FMNUm?usp=sharing) or from GitHub and save them somewhere on your machine.\n",
    "\n",
    "You have three tasks for this problem set, all of which depend on comparing British-authored to American-authored books:\n",
    "\n",
    "1. Calculate the mean frequency per 100,000 words, as well as the upper and lower bounds of a 95% confidence interval, for the terms `['color', 'honor', 'center', 'fish', 'person']` in each national subcorpus\n",
    "    1. Perform this calculation analyticaly, that is, using the observed sample means and standard deviations.\n",
    "    1. Calculate the same quantities via bootstrap, using 1,000 or more iterations.\n",
    "    1. In both cases, print your results in a tabular format.\n",
    "2. Perform a *t*-test to compare the mean frequency of each of these terms between British and American texts. Report the test statistic and *p*-value for each comparison. Note which means are significantly different at the *p*<0.05 level.\n",
    "3. Perform a logistic regression classification of each volume as British or American. \n",
    "    1. Your final features should be the 25 most informative (as measured by the mutual information criterion) token unigrams.\n",
    "    1. Report your 10-fold cross-validated F1 score before and after restricting your input features to the 25 most-informative token types.\n",
    "    1. Calculate the *importance* of the 25 top features for classification as measured by permutation importance.\n",
    "    \n",
    "* See code stubs below for step-by-step guidance. \n",
    "* Consult, too, the lecture notes on explainability and on statistics.\n",
    "* You'll likely also need to consult the scikit-learn documentation along the way.\n",
    "\n",
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "metadata_file = 'amer_brit.csv'\n",
    "corpus_dir = os.path.join('..', '..', 'data', 'classcorpus')\n",
    "terms = ['color', 'honor', 'center', 'fish', 'person']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read metadata (5 points)\n",
    "\n",
    "Read the cleaned, minimal corpus metadata from disk (note the variable `metadata_file` defined in the previous cell). I'd suggest using Pandas, but you're welcome to use whatever method you prefer.\n",
    "\n",
    "Note that the format of the metadata file is:\n",
    "```\n",
    "filename,country,wordcount\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the corpus metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the metadata for one volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count words and normalize (5 points)\n",
    "\n",
    "* Count the target words (indicated in the problem statement) in each volume. \n",
    "* Then, **normalize the count of each word type per 100,000 words** in each volume.\n",
    "*  I'd suggest using a `CountVectorizer` object, but again, you may approach this task however you like. \n",
    "* Make sure you lowercase the input tokens.\n",
    "* Use the word counts supplied in the metadata file for length normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and normalize the target terms in each volume as indicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the normalized term frequencies you just calculated for any three documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate analytic means and 95% confidence intervals (15 points)\n",
    "\n",
    "* For each of the five indicated terms, calculate and display the mean and 95% confidence interval within each national group.\n",
    "*  I suggest using the `tconfint_mean()` method from the `DescrStatsW()` function provided by the `statsmodels` library. See lecture notes for an example of working code.\n",
    "* Format your output (roughly) as follows:\n",
    "\n",
    "```\n",
    "Confidence intervals for: gb\n",
    "     term\t    low\t    mean\t    high\n",
    "   color\t  x.xxxx\t  x.xxxx\t  x.xxxx\n",
    "   [and so on ...]\n",
    "```\n",
    "\n",
    "In this part of the problem, calculate your means and CIs analytically, using the observed statistics of each sample, rather than by bootstrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display analytic means and CIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate bootstrapped means and 95% confidence intervals (15 points)\n",
    "\n",
    "* Calculate the same quantities as above, but this time by bootrap resampling of your data. \n",
    "* Use a minimum of 1,000 trials for each case. \n",
    "* Format your results as in the previous question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *t*-tests (20 points)\n",
    "\n",
    "* Perform a *t*-test comparing the mean frequency of each of the indicated terms in the British and American subsets of the corpus.\n",
    "    * You will perform 5 total tests, comparing, for example, the mean frequency of `color` in British texts to the mean frequency of `color` in American texts. Do not cross-compare words (that is, don't compare the frequency of `color` to that of `honor`, etc.).\n",
    "* Note that the *t*-test takes as input two lists of values. These values are the normalized counts for the feature in question in each volume of a subcorpus. There should thus be one list per subcorpus for each feature. You can produce these lists on the fly as you iterate over your feature data.\n",
    "* Display the test statistic and *p*-value for each comparison. \n",
    "    * Format your output for easy readability (do not just print the raw `ttest_ind` object).\n",
    "* Note which differences are significant at the *p*<0.05 level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-tests\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection (25 points)\n",
    "\n",
    "* Vectorize the corpus as indicated below (freebie)\n",
    "* Standard-scale the resulting feature matrix\n",
    "* Produce a one-dimensional label vector, y, indicating the national origin of each volume in the corpus\n",
    "    * Use `1` to indicate American, `0` for British\n",
    "* Calculate the 10-fold cross-validated classification accuracy and F1 score using a logistic regression classifier on the full input matrix\n",
    "* From the full matrix, select the 25 most-informative features\n",
    "    * Use sklearn's `SelectKBest` function with the  `mutual_info_classif` scoring function to produce a feature matrix that contains just these 25 most-informative features\n",
    "    * Print a list of the names (token labels; for example, 'color') of these 25 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize (freebie)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def pre_proc(x):\n",
    "    '''\n",
    "    Takes a unicode string.\n",
    "    Lowercases, strips accents, and removes some escapes.\n",
    "    Returns a standardized version of the string.\n",
    "    '''\n",
    "    import unicodedata\n",
    "    return unicodedata.normalize('NFKD', x.replace(\"_\", \" \").lower().strip())\n",
    "\n",
    "# Set up vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    input='filename',\n",
    "    encoding='utf-8',\n",
    "    preprocessor=pre_proc,\n",
    "    min_df=11, # Note this\n",
    "    max_df=0.8, # This, too\n",
    "    binary=False,\n",
    "    norm='l2',\n",
    "    max_features=5000,\n",
    "    use_idf=True # And this\n",
    ")\n",
    "\n",
    "# Perform vectorization\n",
    "X = vectorizer.fit_transform(file_list) # <-- MODIFY TO USE THE LIST OF FILES ON YOUR MACHINE\n",
    "\n",
    "# Get the dimensions of the doc-term matrix\n",
    "print(\"Matrix shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard-scale your feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the overall mean of your scaled features (use np.mean(X)).\n",
    "# Should be very close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a one-dimensional vector of true labels for classification\n",
    "# 1='us', 0='gb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using your label vector, display the number of US texts in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freebie function to summarize and display classifier scores\n",
    "def compare_scores(scores_dict, color=True):\n",
    "    '''\n",
    "    Takes a dictionary of cross_validate scores.\n",
    "    Returns a color-coded Pandas dataframe that summarizes those scores.\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(scores_dict).T.applymap(np.mean)\n",
    "    if color:\n",
    "        df = df.style.background_gradient(cmap='RdYlGn')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validate the logistic regression classifier on full input data\n",
    "# Consult PS 6 for useful code\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display your cross-validation results\n",
    "# Use the compare_scores function defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 25 most-informative features as specified above \n",
    "#  and produce a new feature matrix containing only those features\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of your new feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of the features retained in the new feature matrix\n",
    "# Store these feature names in a list, then print the list\n",
    "\n",
    "# Hint: use a combination of your original vectorizer's `.get_feature_names()` method \n",
    "#  and the `SelectKBest` object's `.get_support()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display the 10-fold cross-validated accuracy and F1 of the\n",
    "#  logistic regression using the new, smaller feature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify the 5 most important features (15 points)\n",
    "\n",
    "* Split the new matrix of most-informative features into train (75%) and test (25%) sets (use sklearn's `train_test_split`)\n",
    "* Train a default logistic regression classifier on the training set\n",
    "    * Print the trained model's score on the test set (use the trained classifier's `.score()` method)\n",
    "* Use sklearn's `permutation_importance` function to calculate the importance of each input feature\n",
    "* Print the feature importances from most to least important using the supplied function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the selected feature matrix into train and test sets\n",
    "# Then, train a logistic regression classifier on the train set\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the score of the trained classifier on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance via permutation\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freebie function to print ranked list of feature importances\n",
    "def print_importances(importance_object, feature_names):\n",
    "    '''\n",
    "    Takes a trained permutation_importance object and a list of feature names.\n",
    "    Prints an ordered list of features by descending importance.\n",
    "    '''\n",
    "    for i in importance_object.importances_mean.argsort()[::-1]:\n",
    "\n",
    "        print(f\"{feature_names[i]:<8}\"\n",
    "            f\"\\t{importance_object.importances_mean[i]:.3f}\"\n",
    "            f\" +/- {importance_object.importances_std[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print ranked list of features by permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
